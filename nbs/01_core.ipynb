{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Code\n",
    "\n",
    "> Minimal reprocution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        ID              SIZE      MODIFIED     \n",
      "llava:latest                8dd30f6b0cb1    4.7 GB    8 weeks ago     \n",
      "llama3.1:latest             62757c860e01    4.7 GB    8 weeks ago     \n",
      "mxbai-embed-large:latest    468836162de7    669 MB    2 months ago    \n",
      "codegemma:latest            0c96700aaada    5.0 GB    2 months ago    \n",
      "gemma2:latest               ff02c3702f32    5.4 GB    2 months ago    \n",
      "aya:8b                      7ef8c4942023    4.8 GB    2 months ago    \n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import ollama\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "errors = []\n",
    "try:\n",
    "    installed_models = [model for model in ollama.list()[\"models\"]]\n",
    "    for model in installed_models:\n",
    "        size = model[\"size\"] / (1024 ** 3)\n",
    "        # print(f\"{model[\"name\"]} \\t{size:.2f} GB\")\n",
    "    server_is_available = True\n",
    "except Exception as e:\n",
    "    print(f\"{e}. Is the ollama app running?\")\n",
    "    server_is_available = False\n",
    "    errors.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import inspect\n",
    "from typing import Callable, Dict, List, Tuple, Optional, Any, Union \n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_json_schema(func: Callable) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a JSON schema for the given function based on its annotations and docstring.\n",
    "    \n",
    "    Args:\n",
    "        func (Callable): The function to generate a schema for.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: A JSON schema for the function.\n",
    "    \"\"\"\n",
    "    annotations = func.__annotations__\n",
    "    doc = inspect.getdoc(func)\n",
    "    \n",
    "    schema = {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": \"\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if doc:\n",
    "        lines = doc.split('\\n')\n",
    "        description = []\n",
    "        arg_descriptions = {}\n",
    "        in_args_section = False\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.lower().startswith('args:'):\n",
    "                in_args_section = True\n",
    "                continue\n",
    "            elif line.lower().startswith('returns:') or line.lower().startswith('raises:'):\n",
    "                break\n",
    "            \n",
    "            if not in_args_section:\n",
    "                description.append(line)\n",
    "            else:\n",
    "                parts = line.split(':')\n",
    "                if len(parts) >= 2:\n",
    "                    # print(parts) # Uncomment for debugging\n",
    "                    arg_name = parts[0].split(' ')[0].strip()\n",
    "                    arg_desc = ':'.join(parts[1:]).strip()\n",
    "                    # print(arg_name, arg_desc) # Uncomment for debugging\n",
    "                    arg_descriptions[arg_name] = arg_desc\n",
    "                    # print(arg_descriptions) # Uncomment for debugging\n",
    "                    if 'optional' not in parts[0].lower():\n",
    "                        schema['function']['parameters']['required'].append(arg_name)\n",
    "\n",
    "        \n",
    "        schema['function']['description'] = ' '.join(description).strip()\n",
    "    \n",
    "    for arg, arg_type in annotations.items():\n",
    "        if arg != 'return':\n",
    "            schema['function']['parameters']['properties'][arg] = {\n",
    "                \"type\": _get_type(arg_type),\n",
    "                \"description\": arg_descriptions.get(arg, \"\")\n",
    "            }\n",
    "    \n",
    "    return schema\n",
    "\n",
    "def _get_type(arg_type):\n",
    "    if 'List' in str(arg_type):\n",
    "        return \"list\"\n",
    "    elif 'Optional' in str(arg_type):\n",
    "        return \"optional\"\n",
    "    elif arg_type == int:\n",
    "        return \"integer\"\n",
    "    elif arg_type == str:\n",
    "        return \"string\"\n",
    "    elif arg_type == float:\n",
    "        return \"number\"\n",
    "    elif arg_type == bool:\n",
    "        return \"boolean\"\n",
    "    else:\n",
    "        return \"object\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import functools\n",
    "from typing import TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "T = TypeVar('T', bound=Callable)\n",
    "\n",
    "def json_schema_decorator(func: T) -> T:\n",
    "    \"\"\"\n",
    "    Decorator to generate and attach a JSON schema to a function.\n",
    "    \n",
    "    Args:\n",
    "        func (Callable): The function to decorate.\n",
    "    \n",
    "    Returns:\n",
    "        Callable: The decorated function with an attached JSON schema.\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return func(*args, **kwargs)\n",
    "    \n",
    "    schema = generate_json_schema(func)\n",
    "    wrapper.json_schema = schema  # Attach the schema dictionary directly\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local file processing: listing, content extraction, and summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@json_schema_decorator\n",
    "def get_file_names(ext: str = \"pdf, txt\") -> str:\n",
    "    \"\"\"Retrieves a list of file names with specified extensions in a local data directory the assistant has access to on the user's computer.\n",
    "\n",
    "    Args:\n",
    "        ext: A comma-separated string of file extensions to filter the files by. Options are: pdf, txt, md, markdown, csv, and py. Defaults to \"pdf, txt\".\n",
    "\n",
    "    Returns:\n",
    "        str: A comma-separated string of file names with the specified extensions. If no files are found, a message is returned.\n",
    "\n",
    "    Example:\n",
    "        >>> get_file_names(ext=\"pdf, txt\")\n",
    "        \n",
    "        \"List of file names with the specified extensions in the local data directory: file1.pdf, file2.txt\"\n",
    "    \"\"\"\n",
    "\n",
    "    if 'DIR_PATH' not in globals():\n",
    "        return \"Error: The local data directory path is not defined.\"\n",
    "    \n",
    "    if not os.path.exists(DIR_PATH):\n",
    "        return f\"Error: The local data directory does not exist.\"\n",
    "\n",
    "    valid_extensions = [\"pdf\", \"txt\", \"md\", \"markdown\", \"csv\", \"py\"]\n",
    "\n",
    "    # Process the input extensions\n",
    "    selected_extensions = []\n",
    "    for e in ext.split(','):\n",
    "        e = e.lower().strip().lstrip('.').strip('{').strip('}').strip('[').strip(']').strip('(').strip(')').strip('\"').strip(\"'\") # Clean up the extension string\n",
    "        if e not in valid_extensions:\n",
    "            return f\"Error: Invalid file extension '{e}'. Please choose from: pdf, txt, md, markdown, csv, py.\" # Instead of raising an error, we return a message to the LLM to avoid stopping a conversation\n",
    "        selected_extensions.append(e)\n",
    "\n",
    "    # List all files with the specified extensions\n",
    "    file_names = [file for file in os.listdir(DIR_PATH) if any(file.endswith(f\".{e}\") for e in selected_extensions)]\n",
    "    # print(file_names) # Uncomment for debugging\n",
    "\n",
    "    if len(file_names) == 0:\n",
    "         return \"Access of local data directory successful but no files found with the specified extensions.\"\n",
    "\n",
    "    file_names_json = ', '.join(file_names)\n",
    "\n",
    "    # Convert list to a comma-separated string. This is because the object is returned to the LLM and the API accepts str only\n",
    "    return f\"List of file names with the specified extensions in the local data directory: {file_names_json}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "assert type(get_file_names.json_schema) == dict\n",
    "assert get_file_names.json_schema['function']['name'] == \"get_file_names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@json_schema_decorator\n",
    "def extract_text_from_pdf(file_name: str, page_range: Optional[str] = None) -> str:\n",
    "    \"\"\"A function that extracts text from a PDF file.\n",
    "    Use this tool to extract specific details from a PDF document, such as abstract, authors, conclusions, or user-specified content of other sections.\n",
    "    If the user specifies a page rage, use the optional page_range parameter to extract text from specific pages.\n",
    "    If the user uses words such as beginning, middle, or end, to descripe the section, infer the page range based on the total number of 15 pages in a document.\n",
    "    Do not use this tool to summarize an entire PDF document. Only use this tool for documents with extensions .pdf, or .PDF.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The file name of the PDF document in the local data directory.\n",
    "        page_range (Optional[str]): A string with page numbers and/or page ranges separated by commas (e.g., \"1\" or \"1, 2\", or \"5-7\"). Default is None to extract all pages.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF.\n",
    "\n",
    "    Example:\n",
    "    >>> text = extract_text_from_pdf(\"./test.pdf\", page_range=\"1\")\n",
    "    \"\"\"\n",
    "    verbose = False  # Set to True for debugging\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(DIR_PATH):\n",
    "        return \"Local data directory not found.\"\n",
    "    \n",
    "    # Construct the full file path; this will be handled by a try-except block because this function is alos used as a tool\n",
    "    try:\n",
    "        pdf_path = os.path.join(DIR_PATH, file_name)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    \n",
    "    # Validate input\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"File not found: {pdf_path}\")\n",
    "    assert pdf_path.endswith('.pdf'), \"The file must be a PDF.\"\n",
    "    assert isinstance(page_range, (str, type(None))), \"The page_range must be a string or None.\"\n",
    "\n",
    "    if page_range:\n",
    "        # Clean up page range input in case of LLM formatting errors; must be a string, e.g., \"1\" or \"1, 2\", or \"5-7\", with no quotes or brackets   \n",
    "        page_range = page_range.strip().replace('\"', '').replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "        # Parse the page range string\n",
    "        page_numbers = []\n",
    "        for part in page_range.split(','):\n",
    "            if '-' in part:\n",
    "                a, b = part.split('-')\n",
    "                page_numbers.extend(range(int(a), int(b) + 1))\n",
    "            else:\n",
    "                page_numbers.append(int(part))\n",
    "        if verbose:\n",
    "            print(f\"Extracting text from pages: {page_numbers}\")\n",
    "\n",
    "    # Validate page numbers\n",
    "    if page_range:\n",
    "        start_page, end_page = min(page_numbers), max(page_numbers)\n",
    "        if start_page < 1 or end_page < start_page:\n",
    "            raise ValueError(\"Invalid page range. Please provide a valid range of pages to extract text from.\")\n",
    "\n",
    "    # Extract text from the PDF\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Check for invalid page numbers\n",
    "            if page_range:\n",
    "                max_page = len(pdf.pages)\n",
    "                invalid_pages = [p for p in page_numbers if p < 1 or p > max_page]\n",
    "                if invalid_pages:\n",
    "                    page_range = None # Reset page range to extract all pages\n",
    "            \n",
    "            if page_range:\n",
    "                for page_num in page_numbers:\n",
    "                    if verbose:\n",
    "                        print(f\"Extracting text from page {page_num}...\")\n",
    "                    page = pdf.pages[page_num - 1]  # Adjust for 0-based index\n",
    "                    text += f\"page {page_num} of {len(pdf.pages)}\\n{page.extract_text()}\"\n",
    "            else:\n",
    "                for page_num, page in enumerate(pdf.pages):\n",
    "                    if verbose:\n",
    "                        print(f\"Extracting text from page {page_num + 1}...\")\n",
    "                    text += f\"page {page_num} of {len(pdf.pages)}\\n{page.extract_text()}\"\n",
    "            \n",
    "            if verbose:\n",
    "                word_count = len(text.split())\n",
    "                total_pages = len(page_numbers) if page_range else len(pdf.pages)\n",
    "                print(f\"Text extraction completed.\\nTotal pages extracted: {total_pages}\\nWord count: {word_count}\\nNo. of characters (with spaces): {len(text)}\")\n",
    "    except PyPDF2.errors.PdfReadError as e:\n",
    "        return f\"Error reading PDF: {e}\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_title_and_first_author(contents: List[Dict[str, str]], model: str='llama3.1', verbose: Optional[bool] = False, show_progress: Optional[bool] = False) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    A function that extracts the titles and the first author's names from the text of one or more research articles.\n",
    "\n",
    "    Args:\n",
    "        contents (List[Dict[str, str]]): A list of dictionaries containing the file name and extracted text.\n",
    "        model (str): The model to use for the extraction. Default is 'llama3.1'.\n",
    "        verbose (Optional[bool]): Whether to print additional information. Default is False.\n",
    "        show_progress (Optional[bool]): Whether to show a progress bar. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        contents (List[Dict[str, str]]): The input list of dictionaries with the extracted title and first author added.\n",
    "\n",
    "    Raises:\n",
    "        JSONDecodeError: If the JSON response is invalid.\n",
    "\n",
    "    Example:\n",
    "    >>> contents = extract_title_and_first_author(contents)\n",
    "    Extracting titles and first authors: 100%|██████████| 3/3 [00:22<00:00,  7.35s/it]\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    The text below between the <text> XML like tags is extracted from the first page of a research article. \n",
    "    Your task is to identify the title of the research article and the first author's name.\n",
    "    The title is typically located immediately before the authors' names and the abstract.\n",
    "\n",
    "    <text>\n",
    "    {text}\n",
    "    </text>\n",
    "\n",
    "    The output must be provided in JSON format shown in the following example.\n",
    "\n",
    "    Example output:\n",
    "    {{\n",
    "        \"title\": \"<title>\",\n",
    "        \"first_author\": \"<first_author>\"\n",
    "    }}\n",
    "    Write the JSON output and nothing more. Do not include degree titles or affiliations in the author's name.\n",
    "\n",
    "    Here is the JSON output:\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_iterator = tqdm(contents, desc=\"Extracting titles and first authors\") if show_progress else contents\n",
    "    \n",
    "    for pdf_item in pdf_iterator:\n",
    "        text = pdf_item['extracted_text']\n",
    "        if verbose:\n",
    "            tqdm.write(pdf_item['file_path'])\n",
    "            tqdm.write(\"---\")\n",
    "            tqdm.write(text[:500])\n",
    "            tqdm.write(\"---\")\n",
    "\n",
    "        try:\n",
    "            response = ollama.chat(model=model, messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt.format(text=text),\n",
    "                },\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Error: {e}\")\n",
    "            if \"Connection refused\" in str(e):\n",
    "                tqdm.write(\"Make sure Ollama is running and the correct model is available.\")\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            tqdm.write(response['message']['content'])\n",
    "\n",
    "        try:\n",
    "            extracted_info = json.loads(response['message']['content'])\n",
    "        except json.JSONDecodeError:\n",
    "            tqdm.write(f\"Error: Invalid JSON response for {pdf_item['file_path']}\")\n",
    "            extracted_info = {\"title\": \"\", \"first_author\": \"\"}\n",
    "\n",
    "        pdf_item.update(extracted_info)\n",
    "\n",
    "    print(\"\\n\") if show_progress else None # Add a newline if showing progress bar\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@json_schema_decorator\n",
    "def get_titles_and_first_authors() -> str:\n",
    "    \"\"\"\n",
    "    A function that retrieves the titles of research articles from a directory of PDF files.\n",
    "\n",
    "    Returns:\n",
    "        str: A JSON-formatted string containing the titles, first authors and file names of the research articles.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified directory does not exist.\n",
    "\n",
    "    Example:\n",
    "    >>> get_titles_and_first_authors()\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(DIR_PATH):\n",
    "        return \"Local data directory not found.\"\n",
    "\n",
    "    # Initialize an empty list to store the file names and extracted text\n",
    "    pdf_contents = []\n",
    "\n",
    "    # Initialize an empty list of dictionaries to store file names, extracted titles and first authors\n",
    "    titles_and_authors = []\n",
    "\n",
    "    # Get the file paths of all PDF files in the local data directory\n",
    "    file_paths = [os.path.join(DIR_PATH, file) for file in os.listdir(DIR_PATH) if file.endswith('.pdf')]\n",
    "\n",
    "    # Extract text from the first page of each PDF file\n",
    "    for file_path in file_paths:\n",
    "        pdf_contents.append({\n",
    "            \"file_name\": file_path.split(\"/\")[-1],\n",
    "            \"extracted_text\": extract_text_from_pdf(file_path, page_range=\"1\")\n",
    "            })\n",
    "\n",
    "    # Extract titles and first authors from the extracted text of each PDF file\n",
    "    response = extract_title_and_first_author(pdf_contents, show_progress=True)\n",
    "    for article in response:\n",
    "        titles_and_authors.append({\n",
    "            \"title\": article.get('title'), \n",
    "            \"first_author\": article.get('first_author'),\n",
    "            \"file_name\": article.get('file_name'),\n",
    "            })\n",
    "    \n",
    "    if not titles_and_authors:\n",
    "        return \"No titles found.\"\n",
    "        \n",
    "    return json.dumps(titles_and_authors, indent=2) # Return as a JSON-formatted string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "assert type(get_titles_and_first_authors.json_schema) == dict\n",
    "assert get_titles_and_first_authors.json_schema['function']['name'] == \"get_titles_and_first_authors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@json_schema_decorator\n",
    "def summarize_local_document(file_name: str, ext: str = \"pdf\") -> str:\n",
    "    \"\"\"Summarize the content of a single PDF, markdown, or text document from the local data directory.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The file name of the local document to summarize.\n",
    "        ext (str): The extension of the local document. Options are: pdf, txt, md, and markdown. Defaults to \"pdf\".\n",
    "\n",
    "    Returns:\n",
    "        str: The summary of the content of the local document.\n",
    "\n",
    "    Example:\n",
    "        >>> summarize_local_document(\"research_paper\", ext=\"pdf\")\n",
    "    \"\"\"\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(DIR_PATH):\n",
    "        return \"Local data directory not found.\"\n",
    "\n",
    "    # Ensure the extension is valid: delete spaces, hyphens, and quotation marks, and convert to lowercase in case of LLM input errors\n",
    "    ext = ext.lower().replace('\"', '').replace(\"'\", \"\").replace(\" \", \"\").replace(\".\", \"\")\n",
    "    if ext not in [\"pdf\", \"txt\", \"md\", \"markdown\"]:\n",
    "        return f\"Invalid file extension '{ext}'. Please choose from: pdf, txt, md, markdown.\"\n",
    "\n",
    "    # Get the file paths of all files in the local data directory\n",
    "    file_paths = [os.path.join(DIR_PATH, file) for file in os.listdir(DIR_PATH) if file.endswith(ext)]\n",
    "    # print(file_paths) # Uncomment for debugging\n",
    "    \n",
    "    # Find the file path that matches the specified file name\n",
    "    file_path = [path for path in file_paths if file_name in path]\n",
    "    # print(file_path) # Uncomment for debugging\n",
    "\n",
    "    if not file_path:\n",
    "        return f\"No file found with the name '{file_name}' and extension '{ext}'.\"\n",
    "    elif len(file_path) > 1:\n",
    "        return f\"Multiple files found with the name '{file_name}' and extension '{ext}'. Please specify a unique file name.\"\n",
    "    else:\n",
    "        file_path = file_path[0] # Convert the file path list to a string\n",
    "\n",
    "    if ext == \"pdf\":\n",
    "        # Extract text from a PDF file\n",
    "        try:\n",
    "            full_text = extract_text_from_pdf(file_path, page_range=None)\n",
    "        except Exception as e:\n",
    "            return f\"Error while extracting text from PDF file {file_name}: {e}\"\n",
    "    \n",
    "    if ext in [\"txt\", \"md\", \"markdown\"]:\n",
    "        # Read the full text content from a text file\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                full_text = file.read()\n",
    "        except Exception as e:\n",
    "            return f\"Error while reading the file {file_name}: {e}\"\n",
    "    # print(full_text[:500]) # Uncomment for debugging\n",
    "\n",
    "    # Remove references section from the full text content, if present\n",
    "    patterns = [\"References\", \"REFERENCES\", \"references\", \"Bibliography\", \"BIBLIOGRAPHY\", \"bibliography\"]\n",
    "    for pattern in patterns:\n",
    "        if pattern in full_text:\n",
    "            full_text = full_text.split(pattern)[0]\n",
    "            break\n",
    "\n",
    "    # Summarize the full text content of the document\n",
    "    prompt = f\"\"\"\n",
    "    The text below is the full text content from single document with the file name '{file_name}'.\n",
    "\n",
    "    <text>\n",
    "    {full_text}\n",
    "    </text>\n",
    "\n",
    "    If the document has an abstract, use the abstract for the summary. The abstract is typically located at the beginning of the document (page 1) and provides a concise summary of the research.\n",
    "    If there is no abstract, generate a concise summary (approx. 200 words) that captures the main points of the document, including key findings and conclusions.\n",
    "    Remember that your task is to summarize the content of the main text accurately and concisely, ignoring acknowledgements and references that are typically listed at the end of the document, after the conclusion section.\n",
    "    Start the summary with the title of the document, typically found on the first page before the author names.\n",
    "    \"\"\"\n",
    "\n",
    "    sys_message = \"\"\"You are a scientific summarization assistant for health and life sciences research. \n",
    "    Your task is to condense the contents of a complex research document with mutiple pages into a concise, accurate summary.\n",
    "    If the document describes a research study, highlight the main findings, methodologies, and conclusions of the study.\n",
    "    Start the summary with the title of the document found on the first page, followed by a brief summary of the content.\n",
    "    Ignore references typically listed at the end of a document after the conclusion section, as well as acknowledgements, and other non-content sections.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the model from the global variables\n",
    "    if 'MODEL' in globals():\n",
    "        model = MODEL\n",
    "    else:\n",
    "        model = \"llama3.1\" # Default model\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': \"system\", 'content': sys_message},\n",
    "                {'role': 'user', 'content': prompt},\n",
    "            ]\n",
    "        )\n",
    "        # TODO: Try out different optional parameters for the ollama.chat function, such as temperature, max_tokens, etc. to improve the quality of the summary\n",
    "        # For details, see the Ollama API documentation:\n",
    "        # https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "        # https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n",
    "\n",
    "        summary = response['message']['content']\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error while summarizing the content of the document '{file_name}': {e}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "assert type(summarize_local_document.json_schema) == dict\n",
    "assert summarize_local_document.json_schema['function']['name'] == \"summarize_local_document\"\n",
    "assert 'file_name' in summarize_local_document.json_schema['function']['parameters']['properties'].keys()\n",
    "assert 'ext' in summarize_local_document.json_schema['function']['parameters']['properties'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@json_schema_decorator\n",
    "def describe_python_code(file_name: str) -> str:\n",
    "    \"\"\"Describe the purpose of the Python code in a local Python file.\n",
    "    This may involve summarizing the entire code, extracting key functions, or providing an overview of the code structure.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The file name of the local Python code file document to describe.\n",
    "\n",
    "    Returns:\n",
    "        str: A description of the purpose of the Python code in the local file.\n",
    "\n",
    "    Example:\n",
    "        >>> describe_python_code(\"main.py\", ext=\"py\")\n",
    "    \"\"\"\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(DIR_PATH):\n",
    "        return \"Local data directory not found.\"\n",
    "\n",
    "    # Get the file paths of all files in the local data directory\n",
    "    file_paths = [os.path.join(DIR_PATH, file) for file in os.listdir(DIR_PATH) if file.endswith(\".py\")]\n",
    "    # print(file_paths) # Uncomment for debugging\n",
    "    \n",
    "    # Find the file path that matches the specified file name\n",
    "    file_path = [path for path in file_paths if file_name in path]\n",
    "    # print(file_path) # Uncomment for debugging\n",
    "\n",
    "    if not file_path:\n",
    "        return f\"No file found with the name '{file_name}' and extension '.py'.\"\n",
    "    elif len(file_path) > 1:\n",
    "        return f\"Multiple files found with the name '{file_name}' and extension '.py'. Please specify a unique file name.\"\n",
    "    else:\n",
    "        file_path = file_path[0] # Convert the file path list to a string\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            full_text = file.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error while reading the file {file_name}: {e}\"\n",
    "    # print(full_text[:500]) # Uncomment for debugging\n",
    "\n",
    "    # Summarize the full text content of the document\n",
    "    prompt = f\"\"\"\n",
    "    The text below is the full Python code content from the file '{file_name}'.\n",
    "\n",
    "    <code>\n",
    "    {full_text}\n",
    "    </code>\n",
    "\n",
    "    Your task is to describe the purpose of the Python code in the file. This may involve summarizing the entire code, extracting key functions, or providing an overview of the code structure.\n",
    "    \"\"\"\n",
    "\n",
    "    sys_message = \"\"\"You are a programming assistant for Python code. Your task is to describe the purpose of Python code in a local file for the user who may not be familiar with the code,\n",
    "    and may not know how to interpret the code. If not ask for specific content, provide a high-level overview of the code's functionality, key functions, and structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the model from the global variables\n",
    "    if 'MODEL' in globals():\n",
    "        model = MODEL\n",
    "    else:\n",
    "        model = \"llama3.1\" # Default model\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': \"system\", 'content': sys_message},\n",
    "                {'role': 'user', 'content': prompt},\n",
    "            ]\n",
    "        )\n",
    "        # TODO: Try out different optional parameters for the ollama.chat function, such as temperature, max_tokens, etc. to improve the quality of the summary\n",
    "        # For details, see the Ollama API documentation:\n",
    "        # https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "        # https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n",
    "\n",
    "        summary = response['message']['content']\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error while describing the Python code in the file '{file_name}': {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "assert type(describe_python_code.json_schema) == dict\n",
    "assert describe_python_code.json_schema['function']['name'] == \"describe_python_code\"\n",
    "assert 'file_name' in describe_python_code.json_schema['function']['parameters']['properties'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External data retrieval from NCBI, OpenAlex and Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_id(ids: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    For any article(s) in PubMed Central, find all the corresponding PubMed IDs (PMIDs), digital object identifiers (DOIs), and manuscript IDs (MIDs).\n",
    "    \n",
    "    Args:\n",
    "    ids (List[str]): A list of IDs to convert (max 200 per request).\n",
    "    \n",
    "    Returns:\n",
    "    Str: A JSON-formatted string containing the conversion results.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'EMAIL' in globals():\n",
    "        email = EMAIL\n",
    "    else:\n",
    "        try:\n",
    "            email = os.environ['EMAIL']\n",
    "        except KeyError:\n",
    "            return {\"error\": \"Please provide an email address\"}\n",
    "\n",
    "    # API endpoint\n",
    "    base_url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\"\n",
    "    \n",
    "    # Prepare the IDs string\n",
    "    ids_string = \",\".join(ids)\n",
    "    \n",
    "    # Prepare the parameters\n",
    "    params = {\n",
    "        \"tool\": \"scholaris\",\n",
    "        \"email\": email,\n",
    "        \"ids\": ids_string,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def detect_id_type(id_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect the type of the given ID or title.\n",
    "    \n",
    "    Args:\n",
    "    id_string (str): The ID or title to detect.\n",
    "    \n",
    "    Returns:\n",
    "    str: The detected type ('pmid', 'pmcid', 'doi', 'openalex', 'semantic_scholar', 'potential_title', or 'unknown').\n",
    "    \"\"\"\n",
    "    if re.match(r'^\\d{1,8}$', id_string):\n",
    "        return 'pmid'\n",
    "    elif re.match(r'^PMC\\d+$', id_string):\n",
    "        return 'pmcid'\n",
    "    elif re.match(r'^10\\.\\d{4,9}/[-._;()/:A-Z0-9]+$', id_string, re.I): # Detect DOIs, case-insensitive\n",
    "        return 'doi'\n",
    "    elif re.match(r'^[WAIC]\\d{2,}$', id_string):\n",
    "        return 'openalex'\n",
    "    elif re.match(r'^[0-9a-f]{40}$', id_string): \n",
    "        return 'semantic_scholar'\n",
    "    elif re.match(r'^[A-Z][\\w\\s:,\\-()]{10,150}[.?!]?$', id_string, re.I): # A simple heuristic for detecting titles\n",
    "        return 'potential_title'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "\n",
    "@json_schema_decorator\n",
    "def id_converter_tool(ids: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    For any article(s) in PubMed Central, find all the corresponding PubMed IDs (PMIDs), digital object identifiers (DOIs), and manuscript IDs (MIDs).\n",
    "    Use this tool to convert a list of IDs, such as PMIDs, PMCIDs, or DOIs, and find the corresponding IDs for the same articles.\n",
    "    \n",
    "    Args:\n",
    "    ids (str): A string with a comma-separated list of IDs to convert. Must be PMIDs, PMCIDs, or DOIs. The maximum number of IDs per request is 200.\n",
    "    \n",
    "    Returns:\n",
    "    str: A JSON-formatted string containing the conversion results and the detected ID types.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(ids, str):\n",
    "        ids = ids.replace(\"https://doi.org/\", \"\").replace(\"doi.org/\", \"\") # Remove DOI URL prefixes, if present\n",
    "    \n",
    "    # Convert the input string to a list of IDs\n",
    "    try:\n",
    "        ids = ids.split(\",\")\n",
    "    except AttributeError:\n",
    "        return json.dumps({\"error\": \"Input must be a string of comma-separated IDs\"})\n",
    "\n",
    "    if len(ids) > 200:\n",
    "        return json.dumps({\"error\": \"Input IDs must be no more than 200\"})\n",
    "    \n",
    "    # Detect ID types\n",
    "    id_types = [detect_id_type(id_string) for id_string in ids]\n",
    "    \n",
    "    # Convert IDs\n",
    "    conversion_result = convert_id(ids)\n",
    "    \n",
    "    # Check if conversion_result is already a dictionary (error case)\n",
    "    if isinstance(conversion_result, dict):\n",
    "        parsed_result = conversion_result\n",
    "    else:\n",
    "        # Parse the JSON string result\n",
    "        try:\n",
    "            parsed_result = json.loads(conversion_result)\n",
    "        except json.JSONDecodeError:\n",
    "            return json.dumps({\"error\": \"Failed to parse conversion result\"})\n",
    "    \n",
    "    # Prepare the result\n",
    "    result = {\n",
    "        \"conversion_result\": parsed_result,\n",
    "        \"detected_id_types\": dict(zip(ids, id_types))\n",
    "    }\n",
    "    \n",
    "    return json.dumps(result, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@json_schema_decorator\n",
    "def query_openalex_api(query_param: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve metadata for a given article from OpenAlex, a comprehensive open-access catalog of global research papers.\n",
    "    Use this tool to search the OpenAlex API by using the article title, the PubMed ID (PMID), the PubMed Central ID (PMCID) or the digital object identifier (DOI) of an article as the query parameter. \n",
    "    This tool returns the following metadata:\n",
    "    - the OpenAlex ID\n",
    "    - the digital object identifier (DOI) URL\n",
    "    - Citation count\n",
    "    - The open access status\n",
    "    - URL to the open-access location for the work\n",
    "    - Publication year\n",
    "    - A URL to a website listing works that have cite the article\n",
    "    - The type of the article\n",
    "    Use this tool only if an article title, PubMed ID or DOI is provided by the user or was extracted from a local PDF file and is present in the conversation history.\n",
    "    \n",
    "    Args:\n",
    "        query_param (str): The article title, the PubMed ID (PMID), the PubMed Central ID (PMCID) or the digital object identifier (DOI) of the article to retrieve metadata for. May be provided by the user or extracted from a local PDF file and present in the conversation history.\n",
    "\n",
    "    Returns:\n",
    "        str: A JSON-formatted string including the search results from the OpenAlex database. If no results are found or the API query fails, an appropriate message is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'EMAIL' not in globals():\n",
    "        try:\n",
    "            EMAIL = os.getenv(\"EMAIL\")\n",
    "        except KeyError:\n",
    "            EMAIL = \"\"\n",
    "\n",
    "    # Validate the input\n",
    "    if query_param is None or query_param == \"\":\n",
    "        return json.dumps({\"error\": \"The query parameter must be a non-empty string.\"})\n",
    "    elif not isinstance(query_param, str):\n",
    "        query_param = str(query_param)\n",
    "    \n",
    "    if query_param.startswith(\"https://doi.org/\"):\n",
    "        query_param = query_param.replace(\"https://doi.org/\", \"\")\n",
    "    elif query_param.startswith(\"doi.org/\"):\n",
    "        query_param = query_param.replace(\"doi.org/\", \"\")\n",
    "\n",
    "    # Constants\n",
    "    base_url = \"https://api.openalex.org/\" # Define the base URL for the OpenAlex API\n",
    "    \n",
    "    # Initialize variables\n",
    "    filter = \"\"\n",
    "\n",
    "    if detect_id_type(query_param) == \"potential_title\":\n",
    "        url = f\"{base_url}works?\"\n",
    "        filter = f\"title.search:{query_param}\"\n",
    "    elif detect_id_type(query_param) == \"pmid\":\n",
    "        url = f\"{base_url}works/pmid:{query_param}\"\n",
    "    elif detect_id_type(query_param) == \"doi\":\n",
    "        url = f\"{base_url}works/https://doi.org/{query_param}\"\n",
    "    elif detect_id_type(query_param) == \"pmcid\":\n",
    "        url = f\"{base_url}works/pmcid:{query_param}\"\n",
    "    else:\n",
    "        return json.dumps({\"error\": \"The query parameter must be a valid title, PMID, PMCID, or DOI.\"})\n",
    "        \n",
    "    # Set the query parameters\n",
    "    params = {\n",
    "        \"mailto\": EMAIL,\n",
    "        \"page\": 1,\n",
    "        \"per-page\": 5,\n",
    "        \"select\": \"id,doi,title,publication_year,cited_by_count,cited_by_api_url,open_access,type,type_crossref\",\n",
    "    }\n",
    "    if filter:\n",
    "        params[\"filter\"] = filter # Add the filter parameter for title search\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return {\"error\": f\"Failed to query OpenAlex API. Status code: {response.status_code}\"}\n",
    "\n",
    "    raw_search_results = response.json()\n",
    "\n",
    "    if filter:\n",
    "        number_of_search_matches = raw_search_results['meta']['count']\n",
    "        if len(raw_search_results['results']) == 0:\n",
    "            return \"No results found for the provided title.\"\n",
    "        elif number_of_search_matches > 5:\n",
    "            return \"Error: The search results are more than 5. Please provide a correct title.\"\n",
    "        else:\n",
    "            formatted_results = []\n",
    "            for result in raw_search_results['results']:\n",
    "                formatted_results.append(result)\n",
    "            return json.dumps(formatted_results, indent=2)\n",
    "\n",
    "    \n",
    "    if len(raw_search_results) == 0:\n",
    "        return {\"search result\": \"None\"}\n",
    "    else:\n",
    "        return json.dumps(raw_search_results, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@json_schema_decorator\n",
    "def query_semantic_scholar_api(query_param: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve metadata for a given article from the Semantic Scholar Academic Graph (S2AG), a large knowledge graph of scientific literature that combines data from multiple sources.\n",
    "    Use this tool to query the Semantic Scholar Graph API by using either the article title, the PubMed ID, or the digital object identifier (DOI) to retrieve the following metadata:\n",
    "    - the title\n",
    "    - the publication year\n",
    "    - the abstract\n",
    "    - a tldr (too long, didn't read) summary\n",
    "    - the authors of the article\n",
    "    - the URL to the open-access PDF version of the article, if available\n",
    "    - the journal name\n",
    "    - a url to the article on the Semantic Scholar website\n",
    "    Use this tool only if an article title, PubMed ID or DOI is provided by the user or was extracted from a local PDF file and is present in the conversation history.\n",
    "    \n",
    "    Args:\n",
    "        query_param (str): The article title, the PubMed ID, or the digital object identifier of the article to retrieve metadata for. May be provided by the user or extracted from a local PDF file and present in the conversation history. Do not include the 'https://doi.org/' prefix for DOIs, or keys such as 'DOI', 'PMCID' or 'PMID'. The tool will automatically detect the type of identifier provided.\n",
    "\n",
    "    Returns:\n",
    "        str: A JSON-formatted string including the search results from the Semantic Scholar database. If no results are found or the API query fails, an appropriate message is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Try to get the API key from the environment variables, if available, and define the headers\n",
    "    try:\n",
    "        SEMANTIC_SCHOLAR_API_KEY\n",
    "    except NameError:\n",
    "        SEMANTIC_SCHOLAR_API_KEY = os.environ.get(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "    headers = {'x-api-key': SEMANTIC_SCHOLAR_API_KEY}\n",
    "\n",
    "    # Validate the input\n",
    "    if query_param is None or query_param == \"\":\n",
    "        return json.dumps({\"error\": \"The query parameter must be a non-empty string.\"})\n",
    "    elif not isinstance(query_param, str):\n",
    "        query_param = str(query_param)\n",
    "    \n",
    "    # Clean the query parameter\n",
    "    query_param = query_param.strip().lower() # Convert to lowercase and remove leading/trailing whitespace\n",
    "    if query_param.startswith(\"https://doi.org/\"):\n",
    "        query_param = query_param.replace(\"https://doi.org/\", \"\")\n",
    "    elif query_param.startswith(\"doi.org/\"):\n",
    "        query_param = query_param.replace(\"doi.org/\", \"\")\n",
    "    elif query_param.startswith(\"doi\"):\n",
    "        query_param = query_param.replace(\"doi\", \"\")\n",
    "    elif query_param.startswith(\"pmid\"):\n",
    "        query_param = query_param.replace(\"pmid\", \"\")\n",
    "    elif query_param.startswith(\"pmcid\"):\n",
    "        query_param = query_param.replace(\"pmcid\", \"\")\n",
    "    if detect_id_type(query_param) != \"potential_title\":\n",
    "        query_param = query_param.replace(\":\", \"\") # Remove any colons from the query parameter if it is not a title\n",
    "        query_param = query_param.replace(\" \", \"\") # Remove spaces from the query parameter if it is not a title\n",
    "\n",
    "    # Constants\n",
    "    academicgraph_base_url = \"https://api.semanticscholar.org/graph/v1\"\n",
    "    fields = \"title,year,authors,tldr,abstract,citationCount,openAccessPdf,journal,url\" # The values to retrieve from the API\n",
    "\n",
    "\n",
    "    # Construct the URL based on the identifier type\n",
    "    if detect_id_type(query_param) == \"potential_title\":\n",
    "        url = f\"{academicgraph_base_url}/paper/search/match?query={query_param}&fields={fields}\"\n",
    "    elif detect_id_type(query_param) == \"pmid\":\n",
    "        url = f\"{academicgraph_base_url}/paper/PMID:{query_param}?&fields={fields}\"\n",
    "    elif detect_id_type(query_param) == \"doi\":\n",
    "        url = f\"{academicgraph_base_url}/paper/DOI:{query_param}?&fields={fields}\"\n",
    "    else:\n",
    "        return json.dumps({\"error\": \"The query parameter must be a valid title, PMID, or DOI.\"})\n",
    "    \n",
    "    response = requests.get(url, headers=headers) # Send the API request\n",
    "    # print(response.url) # Uncomment for debugging\n",
    "    time.sleep(2) # sleep for 2 seconds to avoid rate limiting issues\n",
    "    \n",
    "    # Check response status\n",
    "    if response.status_code == 200:\n",
    "        return json.dumps(response.json(), indent=2)\n",
    "    else:\n",
    "        return f\"Error: Failed to query Semantic Scholar API. Status code: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@json_schema_decorator\n",
    "def respond_to_generic_queries() -> str:\n",
    "    \"\"\"\n",
    "    A function to respond to generic questions or queries from the user. Use this tool if no better tool is available.\n",
    "\n",
    "    This tool does not take any arguments.\n",
    "\n",
    "    Returns:\n",
    "        str: A response to a generic question.\n",
    "    \"\"\"\n",
    "\n",
    "    return \"There is no specific tool available to respond this query from the user. State your capabilities based the system message or provide a response based on the conversation history.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assistant class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_response(response: Dict[str, Any] or Generator[Dict[str, Any], None, None]) -> None:\n",
    "    \"\"\"\n",
    "    Print the response from the LLM in a human-readable format.\n",
    "\n",
    "    Args:\n",
    "        response (Dict[str, Any] or Generator[Dict[str, Any], None, None]): The response from the LLM.\n",
    "    \"\"\"\n",
    "    # ANSI escape code for blue and red text\n",
    "    BLUE = \"\\033[94m\"\n",
    "    RED = \"\\033[91m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    if isinstance(response, dict):\n",
    "        print(f\"\\n{BLUE}{response['message']['content']}{RESET}\")\n",
    "        return response['message']['content']\n",
    "\n",
    "    elif isinstance(response, Generator):\n",
    "        print(\"\\n\")\n",
    "        _response = \"\"\n",
    "        for chunk in response:\n",
    "            _response += chunk['message']['content']\n",
    "            print(f\"{BLUE}{chunk['message']['content']}{RESET}\", end='', flush=True)\n",
    "        return _response\n",
    "    \n",
    "    elif response is None:\n",
    "        print(f\"\\n{RED}No response from the LLM.{RESET}\")\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"\\n{RED}nvalid response type. Must be a dictionary or a generator.{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import ollama\n",
    "from typing import Dict, Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self,\n",
    "        sys_message: str or None = None, # The system message for the assistant; if not provided, a default message is used\n",
    "        model: str = \"llama3.1:latest\", # The model to use for the assistant\n",
    "        tools: Dict[str, Any] = { # The tools available to the assistant\n",
    "           \"get_file_names\": get_file_names,\n",
    "           \"extract_text_from_pdf\": extract_text_from_pdf,\n",
    "           \"get_titles_and_first_authors\": get_titles_and_first_authors,\n",
    "           \"summarize_local_document\": summarize_local_document,\n",
    "           \"describe_python_code\": describe_python_code,\n",
    "           \"id_converter_tool\": id_converter_tool,\n",
    "           \"query_openalex_api\": query_openalex_api,\n",
    "           \"query_semantic_scholar_api\": query_semantic_scholar_api,\n",
    "           \"respond_to_generic_queries\": respond_to_generic_queries,\n",
    "        },\n",
    "        add_tools: Dict[str, Any] = {}, # Optional argument to add additional tools to the assistant, when initializing\n",
    "        authentication: Optional[Dict[str, str]] = None, # Authentication credentials for API calls to external services\n",
    "        dir_path: str = \"../data\", # The directory path to which the assistant has access on the local computer\n",
    "        messages: List[Dict[str, str]] = []): # The conversation history\n",
    "        \n",
    "        self.sys_message = sys_message\n",
    "        self.model = model\n",
    "        self.tools = tools\n",
    "        self.tools.update(add_tools) # Add additional tools to the assistant, if provided\n",
    "        if self.tools:\n",
    "            self.tools[\"describe_tools\"] = self.describe_tools # Add the describe_tools function to the tools list for the assistant, if the tools list is not empty\n",
    "        self.authentication = authentication or {}\n",
    "        self.dir_path = Path(dir_path).resolve()\n",
    "        self.messages = messages\n",
    "\n",
    "        # Set global variables\n",
    "        global DIR_PATH\n",
    "        DIR_PATH = self.dir_path\n",
    "        global MODEL\n",
    "        MODEL = self.model\n",
    "        # TODO: Consider allowing the user to set different models for different tasks and tools\n",
    "        # e.g. a model such as llama 3.1 for function calls, command-r-plus for summarization, aya for translation, etc.\n",
    "        \n",
    "        # ANSI escape codes, used for output formatting\n",
    "        GREY = \"\\033[90m\"\n",
    "        BLUE = \"\\033[94m\"\n",
    "        RED = \"\\033[91m\"\n",
    "        RESET = \"\\033[0m\"\n",
    "\n",
    "        # Load the API keys from the environment variables or the authentication dictionary\n",
    "        self.SEMANTIC_SCHOLAR_API_KEY = self.authentication.get(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "        self.EMAIL = self.authentication.get(\"EMAIL\")\n",
    "\n",
    "        if not self.SEMANTIC_SCHOLAR_API_KEY:\n",
    "            self.SEMANTIC_SCHOLAR_API_KEY = os.environ.get(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "            if self.SEMANTIC_SCHOLAR_API_KEY:\n",
    "                print(f\"{GREY}Loaded Semantic Scholar API key from the environment variables.{RESET}\")\n",
    "        if not self.EMAIL:\n",
    "            self.EMAIL = os.environ.get(\"EMAIL\")\n",
    "            if self.EMAIL:\n",
    "                print(f\"{GREY}Loaded email address from the environment variables.{RESET}\")\n",
    "\n",
    "        # Generate the default directory for storing data files if it does not exist\n",
    "        if not os.path.exists(DIR_PATH):\n",
    "            os.mkdir(DIR_PATH)\n",
    "            print(f\"{GREY}Created directory {DIR_PATH} for storing data files.{RESET}\\n\")\n",
    "        else: \n",
    "            print(f\"{GREY}A local directory {DIR_PATH} already exists for storing data files. No of files: {len(os.listdir(DIR_PATH))}{RESET}\\n\")\n",
    "\n",
    "        # Set the default system message if not provided\n",
    "        if not self.sys_message:\n",
    "            self.sys_message =\"\"\"You are an AI assistant specialized in analyzing research articles.\n",
    "        Your role is to provide concise, human-readable responses based on information from tools and conversation history.\n",
    "\n",
    "        Key instructions:\n",
    "        1. Use provided tools to gather information before answering.\n",
    "        2. Interpret tool results and provide clear, concise answers in natural language.\n",
    "        3. If you can't answer with available tools, state this clearly.\n",
    "        4. Don't provide information if tool content is empty.\n",
    "        5. Never include raw JSON, tool outputs, or formatting tags in responses.\n",
    "        6. Format responses as plain text for direct human communication.\n",
    "        7. Use clear formatting (e.g., numbered or bulleted lists) when appropriate.\n",
    "        8. Provide article details (e.g., DOI, citation count) in a conversational manner.\n",
    "\n",
    "        Act as a knowledgeable research assistant, offering clear and helpful information based on available tools and data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the model is available\n",
    "        downloaded_models = []\n",
    "        for model in ollama.list()[\"models\"]:\n",
    "            downloaded_models.append((model[\"name\"].replace(\":latest\", \"\")))\n",
    "        assert self.model.replace(\":latest\", \"\") in downloaded_models, f\"Model {self.model} not found. Please pull the latest version from the server.\"\n",
    "\n",
    "        # Check if the selected model supports tool calling\n",
    "        # for more information, visit https://ollama.com/blog/tool-support\n",
    "        assert self.model.split(\":\")[0] in [\"llama3.1\", \"command-r-plus\", \"mistral-nemo\", \"firefunction-v2\"], f\"Model {self.model} does not support tool calling. Please select a different model.\"\n",
    "\n",
    "        if len(self.tools) == 0:\n",
    "            print(f\"\\033[91mNo tools provided! Please add tools to the assistant.\\033[0m\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Assistant, powered by {self.model.split(':')[0]}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def list_tools(self):\n",
    "        \"List the available tools in the assistant.\"\n",
    "        for tool in self.tools.keys():\n",
    "            print(tool)\n",
    "\n",
    "    def get_tools_schema(self):\n",
    "        \"Return the JSON schema for the available tools.\"\n",
    "        return [func.json_schema for func in self.tools.values()]\n",
    "\n",
    "    @json_schema_decorator\n",
    "    def describe_tools(self) -> str:\n",
    "        \"\"\"Use this tool when asked about the assistant's available tools and capabilities.\n",
    "\n",
    "        Returns:\n",
    "            str: A string with the descriptions of the available tools.\n",
    "        \"\"\"\n",
    "        return f\"Available tools are: {self.get_tools_schema()}\\n State your capabilities based the available tools in a conversational manner.\"\n",
    "        # return f\"{self.pprint_tools()}\\n State your capabilities based the available tools in a conversational manner.\"\n",
    "\n",
    "    def chat(self, prompt: str, show_progress: bool = False, stream_response: bool = True, redirect_output: bool = False):\n",
    "        \"\"\"\n",
    "        Start a conversation with the AI assistant.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The user's prompt or question.\n",
    "            show_progress (bool): Whether to show the step-by-step progress of the fuction calls, including the tool calls and tool outputs. Default is False.\n",
    "            stream_response (bool): Whether to stream the final response from the LLM. Default is True. Automatically set to True if redirect_output is True.\n",
    "            redirect_output (bool): Whether to redirect the output to be compatible with st.write_stream. Default is False.\n",
    "\n",
    "        Returns:\n",
    "            str: The AI assistant's response.\n",
    "        \"\"\"\n",
    "        # At the start of the conversation, if no messages are provided, add the system message and user prompt\n",
    "        if not self.messages:\n",
    "            self.messages = [\n",
    "                {'role': \"system\", 'content': self.sys_message},\n",
    "                {'role': 'user', 'content': prompt},\n",
    "            ]\n",
    "        else:\n",
    "            self.messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "        # Generate JSON schemas for the available tools\n",
    "        tools_schema = self.get_tools_schema()\n",
    "\n",
    "        # Make a request to the LLM to select a tool\n",
    "        if show_progress: print(\"Selecting tools...\\n\")\n",
    "        response = ollama.chat(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            tools=tools_schema,\n",
    "            stream=False, # Set to False to avoid streaming the tool calls\n",
    "        )\n",
    "\n",
    "        # Add the model's response to the conversation history\n",
    "        if response.get('message', {}).get('tool_calls'):\n",
    "            if show_progress: print(response['message']['tool_calls']) # Uncomment for debugging\n",
    "            self.messages.append(\n",
    "                {'role': 'assistant', 'tool_calls': response['message']['tool_calls']}\n",
    "                )\n",
    "        else:\n",
    "            # print(\"LLM response (not added to the conversation history):\", response['message']['content']) # Uncomment for debugging\n",
    "            self.messages.append(\n",
    "                {'role': 'assistant', 'tool_calls': []}\n",
    "                )\n",
    "            print(f\"\\033[91mNo tool calls found in the response. Adding an empty tool_calls list to the conversation history. Aborting...\\033[0m\\n\")\n",
    "            return None # Abort the function if no tool calls are found in the response. Goal is to force the assistant to use a tool. We will generate a tool for generic responses.\n",
    "\n",
    "        # Call the function if a tool is selected\n",
    "        for tool in response['message']['tool_calls']:\n",
    "            # print(\"Arguments:\", tool['function']['arguments'])\n",
    "            function_to_call = self.tools[tool['function']['name']]\n",
    "            if show_progress: print(f\"Calling {tool['function']['name']}() with arguments {tool['function']['arguments']}...\\n\")\n",
    "            args = tool['function']['arguments']\n",
    "\n",
    "            try:\n",
    "                function_response = function_to_call(**args)\n",
    "                # print(f\"Function response type: {type(function_response)}\\n\") # Uncomment for debugging\n",
    "                # print(f\"Function response: {function_response}\\n\") # Uncomment for debugging\n",
    "                function_response = str(function_response) if function_response is not None else \"\"\n",
    "                # assert isinstance(function_response, str), \"Function response must be a string.\"\n",
    "            except Exception as e:\n",
    "                function_response = f\"Error: {e}\"\n",
    "\n",
    "                if show_progress: print(f\"Function response:\\n{function_response}\\n\")\n",
    "            # Add the fucntion response to the conversation history\n",
    "            self.messages.append( \n",
    "                {\n",
    "                    'role': 'tool',\n",
    "                    'content': function_response,\n",
    "                }\n",
    "            ) \n",
    "\n",
    "        if redirect_output: # If the output is to be redirected...\n",
    "            stream_response = True # always use streaming for compatibility with st.write_stream\n",
    "\n",
    "        # Make a second request to the LLM with the tool output to generate a final response\n",
    "        if show_progress: print(\"Generating final response...\")\n",
    "        response = ollama.chat(\n",
    "            model=self.model,\n",
    "            format=\"\", # Set to empty string to avoid JSON formatting; If JSON formatting is needed, set to \"json\"\n",
    "            messages=self.messages,\n",
    "            stream=stream_response, # Set to True , response will be a generator\n",
    "        )\n",
    "        \n",
    "        # Advanced parameters (optional):\n",
    "        # keep_alive: controls how long the model will stay loaded into memory following the request (default: 5m)\n",
    "        # options: additional model parameters listed in the documentation for the Modelfile such as temperature\n",
    "        # for more information, visi:\n",
    "        # https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "        # https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n",
    "        \n",
    "        if redirect_output:\n",
    "            # Return a generator for st.write_stream\n",
    "            def response_generator():\n",
    "                full_content = \"\"\n",
    "                for chunk in response:\n",
    "                    content = chunk['message']['content']\n",
    "                    full_content += content\n",
    "                    yield content\n",
    "                self.messages.append({'role': 'assistant', 'content': full_content})\n",
    "            return response_generator()\n",
    "\n",
    "        else:\n",
    "            if isinstance(response, dict):\n",
    "                content = response['message']['content']\n",
    "                self.messages.append({'role': 'assistant', 'content': content})\n",
    "                print(content)\n",
    "                return content  # Return the content directly, not as a generator\n",
    "                \n",
    "            elif hasattr(response, '__iter__'):  # Check if it's iterable (for streaming)\n",
    "                full_content = \"\"\n",
    "                for chunk in response:\n",
    "                    content = chunk['message']['content']\n",
    "                    full_content += content\n",
    "                    print(content, end='', flush=True)\n",
    "                self.messages.append({'role': 'assistant', 'content': full_content})\n",
    "                return full_content  # Return the full content, not as a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_to_class(Class: type):\n",
    "    \"\"\"Register functions as methods in a class that has already been defined.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@add_to_class(Assistant)\n",
    "def show_conversion_history(self, show_function_calls: bool = False):\n",
    "    \"\"\"Display the conversation history.\n",
    "    \n",
    "    Args:\n",
    "        show_function_calls (bool): Whether to show function calls and returns in the conversation history. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # ANSI escape code for blue and red text\n",
    "    BLUE = \"\\033[94m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    GREY = \"\\033[90m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    for message in self.messages:\n",
    "        if message['role'] != 'system':\n",
    "            if message['role'] == 'user':\n",
    "                print(f\"{BOLD}User:{RESET} {message['content']}\\n\")\n",
    "            elif message['role'] == 'assistant':\n",
    "                if 'content' in message and message['content']:\n",
    "                    print(f\"{BOLD}{BLUE}Assistant response:{RESET} {BLUE}{message['content']}{RESET}\\n\")\n",
    "                if 'tool_calls' in message and message['tool_calls'] and show_function_calls:\n",
    "                    print(f\"{BOLD}{BLUE}Assistant function calls:{RESET} \", end='')\n",
    "                    for tool in message['tool_calls']:\n",
    "                        print(f\"{BLUE}{tool['function']['name']}() with arguments {tool['function']['arguments']}{RESET}\\n\")\n",
    "            elif message['role'] == 'tool' and show_function_calls:\n",
    "                # convert str to list\n",
    "                if isinstance(message['content'], str):\n",
    "                    message['content'] = [message['content']]\n",
    "                for fn_return in message['content']:\n",
    "                    print(f\"{BOLD}{GREY}Function return:{RESET} {GREY}{fn_return}{RESET}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@add_to_class(Assistant)\n",
    "def clear_conversion_history(self):\n",
    "    \"\"\"Clear the conversation history.\"\"\"\n",
    "    self.messages = [{'role': \"system\", 'content': self.sys_message},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@add_to_class(Assistant)\n",
    "def pprint_tools(self):\n",
    "    for tool in self.get_tools_schema():   \n",
    "        print(f\"\"\"* Tool name: {tool.get(\"function\", {}).get(\"name\", \"No name available.\")}\n",
    "    Description: {tool.get(\"function\", {}).get(\"description\", \"No description available.\")}\n",
    "        \"\"\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors and exceptions.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Print all errors and exceptions, if any\n",
    "if len(errors) > 0:    \n",
    "    for (error) in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"No errors and exceptions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# remember to save the notebook before running this command\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
